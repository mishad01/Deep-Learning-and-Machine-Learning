{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishad01/Deep-Learning-and-Machine-Learning/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhz0YEEXLpFz",
        "outputId": "b053310f-b561-4cfc-c940-333d0032cae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ESc2t8pdFoLs"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(10)\n",
        "\n",
        "# Define image size and batch size\n",
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "iE25WpR31k9r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to read and preprocess an image\n",
        "def read_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
        "    return image"
      ],
      "metadata": {
        "id": "oUOrOm1411ET"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for random cropping of images\n",
        "def random_crop(low_image, enhanced_image):\n",
        "    # Randomly choose crop coordinates\n",
        "    low_image_shape = tf.shape(low_image)[:2]\n",
        "    low_w = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
        "    )\n",
        "    low_h = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
        "    )\n",
        "    enhanced_w = low_w\n",
        "    enhanced_h = low_h\n",
        "    # Crop the images\n",
        "    low_image_cropped = low_image[\n",
        "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
        "    ]\n",
        "    enhanced_image_cropped = enhanced_image[\n",
        "        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n",
        "    ]\n",
        "    return low_image_cropped, enhanced_image_cropped"
      ],
      "metadata": {
        "id": "EQ7vLfZz2SP0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_data(low_light_image_path, enhanced_image_path):\n",
        "    low_light_image = read_image(low_light_image_path)\n",
        "    enhanced_image = read_image(enhanced_image_path)\n",
        "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
        "    return low_light_image, enhanced_image"
      ],
      "metadata": {
        "id": "KmA34Reu2aBj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oYOglbtrFwJ9"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from file paths\n",
        "def get_dataset(low_light_images, enhanced_images):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for training, validation, and test datasets\n",
        "train_low_light_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/training-set/low/*\"))\n",
        "train_enhanced_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/training-set/high/*\"))\n",
        "val_low_light_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/validation-set/low/*\"))\n",
        "val_enhanced_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/validation-set/high/*\"))\n",
        "test_low_light_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/evaluation-set/low/*\"))\n",
        "test_enhanced_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/evaluation-set/high/*\"))\n"
      ],
      "metadata": {
        "id": "I0QrgFIezZgG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIWIch_OK-qQ",
        "outputId": "c62cc9c7-db2a-42db-ee0e-aca1ec03b6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n",
            "Val Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Create datasets for training and validation\n",
        "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
        "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ujn1xnTLStSH"
      },
      "outputs": [],
      "source": [
        "# Define the selective kernel feature fusion function\n",
        "def selective_kernel_feature_fusion(\n",
        "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
        "):\n",
        "    channels = list(multi_scale_feature_1.shape)[-1]\n",
        "    combined_feature = layers.Add()(\n",
        "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
        "    )\n",
        "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
        "    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n",
        "    compact_feature_representation = layers.Conv2D(\n",
        "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
        "    )(channel_wise_statistics)\n",
        "    feature_descriptor_1 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_descriptor_2 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_descriptor_3 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
        "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
        "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
        "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
        "    return aggregated_feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define spatial attention block - FIXED VERSION\n",
        "def spatial_attention_block(input_tensor):\n",
        "    # Use Lambda layers instead of raw TensorFlow operations\n",
        "    average_pooling = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(input_tensor)\n",
        "    max_pooling = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(input_tensor)\n",
        "    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n",
        "    feature_map = layers.Conv2D(1, kernel_size=(1, 1), activation='sigmoid')(concatenated)\n",
        "    return layers.Multiply()([input_tensor, feature_map])"
      ],
      "metadata": {
        "id": "V66aakGI5hIv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define channel attention block - FIXED VERSION\n",
        "def channel_attention_block(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
        "    # Use Lambda layer instead of tf.reshape\n",
        "    feature_descriptor = layers.Lambda(lambda x: tf.reshape(x, shape=(-1, 1, 1, channels)))(average_pooling)\n",
        "    feature_activations = layers.Conv2D(\n",
        "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
        "    )(feature_descriptor)\n",
        "    feature_activations = layers.Conv2D(\n",
        "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
        "    )(feature_activations)\n",
        "    return layers.Multiply()([input_tensor, feature_activations])"
      ],
      "metadata": {
        "id": "bCf2y6hN_cFf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MMgVznbeS4AW"
      },
      "outputs": [],
      "source": [
        "# Define dual attention unit block\n",
        "def dual_attention_unit_block(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    feature_map = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(input_tensor)\n",
        "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
        "        feature_map\n",
        "    )\n",
        "    channel_attention = channel_attention_block(feature_map)\n",
        "    spatial_attention = spatial_attention_block(feature_map)\n",
        "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
        "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
        "    return layers.Add()([input_tensor, concatenation])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define down-sampling module\n",
        "def down_sampling_module(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
        "        input_tensor\n",
        "    )\n",
        "    main_branch = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(main_branch)\n",
        "    main_branch = layers.MaxPooling2D()(main_branch)\n",
        "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
        "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
        "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
        "    return layers.Add()([skip_branch, main_branch])\n"
      ],
      "metadata": {
        "id": "BvAZq0FB_mW_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define up-sampling module\n",
        "def up_sampling_module(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
        "        input_tensor\n",
        "    )\n",
        "    main_branch = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(main_branch)\n",
        "    main_branch = layers.UpSampling2D()(main_branch)\n",
        "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
        "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
        "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
        "    return layers.Add()([skip_branch, main_branch])\n"
      ],
      "metadata": {
        "id": "-acRYF3BAaFP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GFsmsZ_cS5MW"
      },
      "outputs": [],
      "source": [
        "# Define multi-scale residual block\n",
        "def multi_scale_residual_block(input_tensor, channels):\n",
        "    \"\"\"Multi-scale Residual Block with proper Keras layers\"\"\"\n",
        "    # Multi-scale features\n",
        "    level1 = input_tensor\n",
        "    level2 = down_sampling_module(input_tensor)\n",
        "    level3 = down_sampling_module(level2)\n",
        "\n",
        "    # Dual Attention Units\n",
        "    level1_dau = dual_attention_unit_block(level1)\n",
        "    level2_dau = dual_attention_unit_block(level2)\n",
        "    level3_dau = dual_attention_unit_block(level3)\n",
        "\n",
        "    # Selective Kernel Feature Fusion\n",
        "    level1_skff = selective_kernel_feature_fusion(\n",
        "        level1_dau,\n",
        "        up_sampling_module(level2_dau),\n",
        "        up_sampling_module(up_sampling_module(level3_dau)),\n",
        "    )\n",
        "    level2_skff = selective_kernel_feature_fusion(\n",
        "        down_sampling_module(level1_dau),\n",
        "        level2_dau,\n",
        "        up_sampling_module(level3_dau)\n",
        "    )\n",
        "    level3_skff = selective_kernel_feature_fusion(\n",
        "        down_sampling_module(down_sampling_module(level1_dau)),\n",
        "        down_sampling_module(level2_dau),\n",
        "        level3_dau,\n",
        "    )\n",
        "\n",
        "    # Second Dual Attention Units\n",
        "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
        "    level2_dau_2 = up_sampling_module(dual_attention_unit_block(level2_skff))\n",
        "    level3_dau_2 = up_sampling_module(up_sampling_module(dual_attention_unit_block(level3_skff)))\n",
        "\n",
        "    # Final Selective Kernel Feature Fusion\n",
        "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n",
        "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
        "\n",
        "    return layers.Add()([input_tensor, conv])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define recursive residual group\n",
        "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
        "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
        "    for _ in range(num_mrb):\n",
        "        conv1 = multi_scale_residual_block(conv1, channels)\n",
        "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
        "    return layers.Add()([conv2, input_tensor])"
      ],
      "metadata": {
        "id": "umbAEVwxzxFg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dckfmRxnS94e"
      },
      "outputs": [],
      "source": [
        "# Define the MIRNet model architecture\n",
        "def mirnet_model(num_rrg, num_mrb, channels):\n",
        "    input_tensor = keras.Input(shape=[None, None, 3])\n",
        "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
        "    for _ in range(num_rrg):\n",
        "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
        "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
        "    output_tensor = layers.Add()([input_tensor, conv])\n",
        "    return keras.Model(input_tensor, output_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom loss function (Charbonnier loss) and PSNR metric\n",
        "def charbonnier_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
        "\n",
        "def peak_signal_noise_ratio(y_true, y_pred):\n",
        "    return tf.image.psnr(y_pred, y_true, max_val=255.0)"
      ],
      "metadata": {
        "id": "GjTuLgqQ3qDv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(\n",
        "    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n",
        ")"
      ],
      "metadata": {
        "id": "XVPdlADA3tvW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b91d8cf2-d6bb-45b9-cf4b-ecb0cbd6da95"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1780771415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.compile(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharbonnier_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeak_signal_noise_ratio\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_peak_signal_noise_ratio\",\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=1,\n",
        "            min_delta=1e-7,\n",
        "            mode=\"max\",\n",
        "        )\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "r5-Z0INx4AiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save(\"./gdrive/My Drive/image-enhancement/enhancement_model.h5\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "NotLReJ94Un_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PphrRyHf4q7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsLZblTjTAxF"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation PSNR\n",
        "plt.plot(history.history[\"peak_signal_noise_ratio\"], label=\"train_psnr\")\n",
        "plt.plot(history.history[\"val_peak_signal_noise_ratio\"], label=\"val_psnr\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"Train and Validation PSNR Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot original, enhanced, and PIL autocontrast images\n",
        "def plot_results(images, titles, figure_size=(12, 12)):\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
        "        _ = plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5zlq_kLC4-tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmsmKWLOTNMX"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform image enhancement using the trained model\n",
        "def infer(original_image):\n",
        "    image = keras.utils.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output = model.predict(image)\n",
        "    output_image = output[0] * 255.0\n",
        "    output_image = output_image.clip(0, 255)\n",
        "    output_image = output_image.reshape(\n",
        "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
        "    )\n",
        "    output_image = Image.fromarray(np.uint8(output_image))\n",
        "    original_image = Image.fromarray(np.uint8(original_image))\n",
        "    return output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cQ0aDo8TRrl"
      },
      "outputs": [],
      "source": [
        "# Generate enhanced images from test images and display results\n",
        "for low_light_image in random.sample(test_low_light_images, 6):\n",
        "    original_image = Image.open(low_light_image)\n",
        "    enhanced_image = infer(original_image)\n",
        "    plot_results(\n",
        "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
        "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
        "        (20, 12),\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}