{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishad01/Deep-Learning-and-Machine-Learning/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhz0YEEXLpFz",
        "outputId": "52eba5b9-3453-46fa-80e4-10409782e8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_XH-K0GdWKs",
        "outputId": "f770c37c-bc43-4584-94e7-612538edf748"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ESc2t8pdFoLs"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(10)\n",
        "\n",
        "# Define image size and batch size\n",
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "iE25WpR31k9r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to read and preprocess an image\n",
        "def read_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
        "    return image"
      ],
      "metadata": {
        "id": "oUOrOm1411ET"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for random cropping of images\n",
        "def random_crop(low_image, enhanced_image):\n",
        "    # Randomly choose crop coordinates\n",
        "    low_image_shape = tf.shape(low_image)[:2]\n",
        "    low_w = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
        "    )\n",
        "    low_h = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
        "    )\n",
        "    enhanced_w = low_w\n",
        "    enhanced_h = low_h\n",
        "    # Crop the images\n",
        "    low_image_cropped = low_image[\n",
        "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
        "    ]\n",
        "    enhanced_image_cropped = enhanced_image[\n",
        "        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n",
        "    ]\n",
        "    return low_image_cropped, enhanced_image_cropped"
      ],
      "metadata": {
        "id": "EQ7vLfZz2SP0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_data(low_light_image_path, enhanced_image_path):\n",
        "    low_light_image = read_image(low_light_image_path)\n",
        "    enhanced_image = read_image(enhanced_image_path)\n",
        "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
        "    return low_light_image, enhanced_image"
      ],
      "metadata": {
        "id": "KmA34Reu2aBj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oYOglbtrFwJ9"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from file paths\n",
        "def get_dataset(low_light_images, enhanced_images):\n",
        "    def _load_data_wrapper(low_path, high_path):\n",
        "        low_image, high_image = tf.py_function(\n",
        "            func=load_data,\n",
        "            inp=[low_path, high_path],\n",
        "            Tout=[tf.float32, tf.float32],\n",
        "        )\n",
        "        low_image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "        high_image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "        return low_image, high_image\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
        "    dataset = dataset.map(_load_data_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for training, validation, and test datasets\n",
        "train_low_light_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/training-set/low/*\"))\n",
        "train_enhanced_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/training-set/high/*\"))\n",
        "val_low_light_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/validation-set/low/*\"))\n",
        "val_enhanced_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/validation-set/high/*\"))\n",
        "test_low_light_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/evaluation-set/low/*\"))\n",
        "test_enhanced_images = sorted(glob(\"/content/gdrive/MyDrive/Thesis Dataset/evaluation-set/high/*\"))\n"
      ],
      "metadata": {
        "id": "I0QrgFIezZgG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
        "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN-beRqIelLx",
        "outputId": "cdf999e0-9fe2-4be9-e81e-4a0c1dfe4cb1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None))>\n",
            "Val Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIWIch_OK-qQ",
        "outputId": "d4517d89-12a1-40bc-d189-3a3b48cbf880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None))>\n",
            "Val Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 128, 128, 3), dtype=tf.float32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Create datasets for training and validation\n",
        "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
        "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ujn1xnTLStSH"
      },
      "outputs": [],
      "source": [
        "# Define the selective kernel feature fusion function\n",
        "def selective_kernel_feature_fusion(\n",
        "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
        "):\n",
        "    channels = list(multi_scale_feature_1.shape)[-1]\n",
        "    combined_feature = layers.Add()(\n",
        "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
        "    )\n",
        "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
        "    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n",
        "    compact_feature_representation = layers.Conv2D(\n",
        "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
        "    )(channel_wise_statistics)\n",
        "    feature_descriptor_1 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_descriptor_2 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_descriptor_3 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
        "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
        "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
        "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
        "    return aggregated_feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define spatial attention block\n",
        "def spatial_attention_block(input_tensor):\n",
        "    average_pooling = tf.reduce_max(input_tensor, axis=-1)\n",
        "    average_pooling = tf.expand_dims(average_pooling, axis=-1)\n",
        "    max_pooling = tf.reduce_mean(input_tensor, axis=-1)\n",
        "    max_pooling = tf.expand_dims(max_pooling, axis=-1)\n",
        "    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n",
        "    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(concatenated)\n",
        "    feature_map = tf.nn.sigmoid(feature_map)\n",
        "    return input_tensor * feature_map"
      ],
      "metadata": {
        "id": "V66aakGI5hIv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define channel attention block\n",
        "def channel_attention_block(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
        "    feature_descriptor = tf.reshape(average_pooling, shape=(-1, 1, 1, channels))\n",
        "    feature_activations = layers.Conv2D(\n",
        "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
        "    )(feature_descriptor)\n",
        "    feature_activations = layers.Conv2D(\n",
        "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
        "    )(feature_activations)\n",
        "    return input_tensor * feature_activations"
      ],
      "metadata": {
        "id": "bCf2y6hN_cFf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "MMgVznbeS4AW"
      },
      "outputs": [],
      "source": [
        "# Define dual attention unit block\n",
        "def dual_attention_unit_block(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    feature_map = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(input_tensor)\n",
        "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
        "        feature_map\n",
        "    )\n",
        "    channel_attention = channel_attention_block(feature_map)\n",
        "    spatial_attention = spatial_attention_block(feature_map)\n",
        "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
        "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
        "    return layers.Add()([input_tensor, concatenation])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define down-sampling module\n",
        "def down_sampling_module(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
        "        input_tensor\n",
        "    )\n",
        "    main_branch = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(main_branch)\n",
        "    main_branch = layers.MaxPooling2D()(main_branch)\n",
        "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
        "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
        "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
        "    return layers.Add()([skip_branch, main_branch])"
      ],
      "metadata": {
        "id": "BvAZq0FB_mW_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define up-sampling module\n",
        "def up_sampling_module(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
        "        input_tensor\n",
        "    )\n",
        "    main_branch = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(main_branch)\n",
        "    main_branch = layers.UpSampling2D()(main_branch)\n",
        "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
        "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
        "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
        "    return layers.Add()([skip_branch, main_branch])"
      ],
      "metadata": {
        "id": "-acRYF3BAaFP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GFsmsZ_cS5MW"
      },
      "outputs": [],
      "source": [
        "# Define multi-scale residual block\n",
        "def multi_scale_residual_block(input_tensor, channels):\n",
        "    # features\n",
        "    level1 = input_tensor\n",
        "    level2 = down_sampling_module(input_tensor)\n",
        "    level3 = down_sampling_module(level2)\n",
        "    # DAU\n",
        "    level1_dau = dual_attention_unit_block(level1)\n",
        "    level2_dau = dual_attention_unit_block(level2)\n",
        "    level3_dau = dual_attention_unit_block(level3)\n",
        "    # SKFF\n",
        "    level1_skff = selective_kernel_feature_fusion(\n",
        "        level1_dau,\n",
        "        up_sampling_module(level2_dau),\n",
        "        up_sampling_module(up_sampling_module(level3_dau)),\n",
        "    )\n",
        "    level2_skff = selective_kernel_feature_fusion(\n",
        "        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n",
        "    )\n",
        "    level3_skff = selective_kernel_feature_fusion(\n",
        "        down_sampling_module(down_sampling_module(level1_dau)),\n",
        "        down_sampling_module(level2_dau),\n",
        "        level3_dau,\n",
        "    )\n",
        "    # DAU 2\n",
        "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
        "    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n",
        "    level3_dau_2 = up_sampling_module(\n",
        "        up_sampling_module(dual_attention_unit_block(level3_skff))\n",
        "    )\n",
        "    # SKFF 2\n",
        "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n",
        "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
        "    return layers.Add()([input_tensor, conv])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define recursive residual group\n",
        "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
        "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
        "    for _ in range(num_mrb):\n",
        "        conv1 = multi_scale_residual_block(conv1, channels)\n",
        "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
        "    return layers.Add()([conv2, input_tensor])"
      ],
      "metadata": {
        "id": "umbAEVwxzxFg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "dckfmRxnS94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "1277241c-b980-49b1-87bc-64bf04192ed0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ac7f1eaa3848>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmirnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rrg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mrb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-ac7f1eaa3848>\u001b[0m in \u001b[0;36mmirnet_model\u001b[0;34m(num_rrg, num_mrb, channels)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecursive_residual_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mrb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-78ab8c5fd2cb>\u001b[0m in \u001b[0;36mrecursive_residual_group\u001b[0;34m(input_tensor, num_mrb, channels)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_mrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_scale_residual_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-dc4591f9a595>\u001b[0m in \u001b[0;36mmulti_scale_residual_block\u001b[0;34m(input_tensor, channels)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlevel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown_sampling_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# DAU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlevel1_dau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_attention_unit_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlevel2_dau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_attention_unit_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlevel3_dau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_attention_unit_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-0517ba115c77>\u001b[0m in \u001b[0;36mdual_attention_unit_block\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfeature_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mchannel_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchannel_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mspatial_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mconcatenation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_attention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-708bda323c85>\u001b[0m in \u001b[0;36mchannel_attention_block\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maverage_pooling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfeature_descriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     feature_activations = layers.Conv2D(\n\u001b[1;32m      7\u001b[0m         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;34m\"A KerasTensor cannot be used as input to a TensorFlow function. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;34m\"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
          ]
        }
      ],
      "source": [
        "# Define the MIRNet model architecture\n",
        "def mirnet_model(num_rrg, num_mrb, channels):\n",
        "    input_tensor = keras.Input(shape=[None, None, 3])\n",
        "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
        "    for _ in range(num_rrg):\n",
        "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
        "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
        "    output_tensor = layers.Add()([input_tensor, conv])\n",
        "    return keras.Model(input_tensor, output_tensor)\n",
        "model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom loss function (Charbonnier loss) and PSNR metric\n",
        "def charbonnier_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
        "\n",
        "def peak_signal_noise_ratio(y_true, y_pred):\n",
        "    return tf.image.psnr(y_pred, y_true, max_val=255.0)"
      ],
      "metadata": {
        "id": "GjTuLgqQ3qDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(\n",
        "    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n",
        ")"
      ],
      "metadata": {
        "id": "XVPdlADA3tvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_peak_signal_noise_ratio\",\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=1,\n",
        "            min_delta=1e-7,\n",
        "            mode=\"max\",\n",
        "        )\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "r5-Z0INx4AiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save(\"./gdrive/My Drive/image-enhancement/enhancement_model.h5\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "NotLReJ94Un_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PphrRyHf4q7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsLZblTjTAxF"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation PSNR\n",
        "plt.plot(history.history[\"peak_signal_noise_ratio\"], label=\"train_psnr\")\n",
        "plt.plot(history.history[\"val_peak_signal_noise_ratio\"], label=\"val_psnr\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"Train and Validation PSNR Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot original, enhanced, and PIL autocontrast images\n",
        "def plot_results(images, titles, figure_size=(12, 12)):\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
        "        _ = plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5zlq_kLC4-tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmsmKWLOTNMX"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform image enhancement using the trained model\n",
        "def infer(original_image):\n",
        "    image = keras.utils.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output = model.predict(image)\n",
        "    output_image = output[0] * 255.0\n",
        "    output_image = output_image.clip(0, 255)\n",
        "    output_image = output_image.reshape(\n",
        "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
        "    )\n",
        "    output_image = Image.fromarray(np.uint8(output_image))\n",
        "    original_image = Image.fromarray(np.uint8(original_image))\n",
        "    return output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cQ0aDo8TRrl"
      },
      "outputs": [],
      "source": [
        "# Generate enhanced images from test images and display results\n",
        "for low_light_image in random.sample(test_low_light_images, 6):\n",
        "    original_image = Image.open(low_light_image)\n",
        "    enhanced_image = infer(original_image)\n",
        "    plot_results(\n",
        "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
        "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
        "        (20, 12),\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}